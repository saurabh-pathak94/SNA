
# First trains logistic regression by using input from a group of vertex similarity measures.
# Secondly edge predictions are generated by using the same input data set. 

# Note that since the learning method learns also the places where the edges are NOT connected in the 
# training data, the predictions may also omit some edges that may be existing in the actual (not sampled) graph.
# If the model overfits then it does not predict anything and is therefore likely less useful than its input 
# similarity measures.

# The model is useful when it is able to generalize in such a manner that its output is more likely to be correct
# for some amount of its first best predictions, than it is correct for sorted input similarity measures.

# The more one takes predictions from the prediction set, the more the prediction correctness will degrade.
# That in turn means that the total area under ROC curve should be roughly same for logistic regression and
# for other similarity measures, but the shape should be different.

glmTrainAndPredict <- function(sampledGraph)
{
	source("createTrainData.R")
	
	list <- createTrainData(sampledGraph)
	
	counts <- list[, 1]
	in1 <- list[, 4]
	in2 <- list[, 5]
	in3 <- list[, 6]
	
	trainedModel <- glm(formula = counts ~ in1 + in2 + in3, family = binomial(link = "logit"), na.action = na.pass)		# linear model
	# trainedModel <- glm(formula = counts ~ in1 * in2 * in3, family = binomial(link = "logit"), na.action = na.pass)	# enable nonlinear interactions


	newdata1 <- data.frame(in1, in2, in3)
	
	
	
	res <- predict(trainedModel, newdata=newdata1, type="response")
	
	
	res
	

}

